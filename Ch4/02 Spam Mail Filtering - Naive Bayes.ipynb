{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "02 Spam Mail Filtering - Naive Bayes.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishkalavallabhi/practicalnlp/blob/V_2_0/Ch4/02%20Spam%20Mail%20Filtering%20-%20Naive%20Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mIjEvX0atBo",
        "colab_type": "text"
      },
      "source": [
        "# <font color=blue>*Natural language Processing Applications - Example Codes*</font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_0OIa5KatBr",
        "colab_type": "text"
      },
      "source": [
        "# *Chapter-4 $<$Name of Chapter$>$* \n",
        "## Welcome to the second notebook of Chapter-4. This notebook aims to give you a brief overview of *Spam Filtering* and Naive Bayes Classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr_O37TXatBt",
        "colab_type": "text"
      },
      "source": [
        "# <font color= blue> Spam Filtering Using Naive Bayes. </font>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0FaGI4vatBu",
        "colab_type": "text"
      },
      "source": [
        "Mails are one of the major ways of official communications and out mail servers are having algorithm to tag any incoming mail as spam and move the same to a certain folder named \"SPAM Folder\"\n",
        "Below we are going to see an experiment on the similar case and taking some sample mail we shall build a supervised classification model to detect the given mail is spam or not.\n",
        "\n",
        "#### We are using use Enron dataset for this experimentation: \n",
        "##### Dataset Link: http://www2.aueb.gr/users/ion/data/enron-spam/\n",
        "\n",
        "It contains a preclassified text files containing mails. \n",
        "### Please feel free to download and experiment on the same. Please download and save the data by creating a folder named   \"dataset\"\n",
        "\n",
        "#### Let's import few necessary packages before we start our work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-g8ZJSgatBv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "62b10035-a7ec-4523-d1ff-443ff9881137"
      },
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
        "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
        "from sklearn.metrics import confusion_matrix \n",
        "import numpy as np \n",
        "import scipy as sp \n",
        "import matplotlib as mpl \n",
        "import matplotlib.cm as cm \n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd \n",
        "import nltk\n",
        "import re\n",
        "import csv\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "#from sklearn.cross_validation import train_test_split\n",
        "\n",
        "import sklearn\n",
        "#from sklearn.cross_validation import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from time import time\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ec91df821d9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validation'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkyEsjZnatB3",
        "colab_type": "text"
      },
      "source": [
        "### Section 1: Load the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO1WGY8hatB4",
        "colab_type": "text"
      },
      "source": [
        "#### Generally you may receive multiple text files on Enron dataset which needs to be merged and leveled for further analysis. We are going to create a data frame file containing two columns. Data frame is just like our excel file with different columns and rows. Column represents the type of field and rows represents the data element \n",
        "        Text -> This column would contain the text body of each mail\n",
        "        Class -> This column contains the lebel of whether the mail is 'spam' or 'ham'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNIhtMGxatB6",
        "colab_type": "text"
      },
      "source": [
        "#### First step is to create a data-frame with two columns: \n",
        "<br>\n",
        "* <b>Text</b>: The extracted texts from each mail. \n",
        "* <b>Class</b>: Represents whether it is \"Spam\" and \"Ham"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V7APsfyatB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spam_data = pd.DataFrame(columns = (\"Text\", \"Class\")) #this will create an empty data frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR5tE2MMatCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for cla in glob.glob(\"Datasets/*\"): # Here the folder name is data set in which there are two sub-folder \"Spam\" & \"Ham\"\n",
        "    clas = cla.split(os.sep)[1]    # We are splitting the folder names as class using OS-Seperator and taking the 2nd item in the list\n",
        "    for file in glob.glob(cla + \"/*.txt\"): # Here we are deep diving in each of the folder and reading the text files one by one\n",
        "        text = open(file, \"r\", encoding = \"ISO-8859-1\").read() # Reading the file , for Windows generally we need to mention the encoding \n",
        "        text = \" \".join(text.split(\"\\n\")) # Splitting the text files and rejoining into a single text\n",
        "        spam_data = spam_data.append(pd.Series([text, clas], index = [\"Text\", \"Class\"]), ignore_index = True) # continious append to the data frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxI5Fo4EatCG",
        "colab_type": "text"
      },
      "source": [
        "### Section 2: Explore the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzPxfS8HatCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spam_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goUvKoD-atCO",
        "colab_type": "text"
      },
      "source": [
        "### Lets check the distribution of classes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvAEhdMFatCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spam_data['Class'].value_counts() / spam_data.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlZDyQmFatCV",
        "colab_type": "text"
      },
      "source": [
        "#### There is slight imbalance in the data with *Ham* being 70% in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ9sCilDatCW",
        "colab_type": "text"
      },
      "source": [
        "#### Converting the class label into binary outcome variable for convenience.\n",
        "* 1- in case the mail is a spam mail\n",
        "\n",
        "* 0- in case the mail is not a spam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHOSr8buatCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert label to a numerical variable\n",
        "spam_data['Class'] = spam_data.Class.map({'ham':0, 'spam':1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILz4-3_FatCe",
        "colab_type": "text"
      },
      "source": [
        "### Section 3: Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCwJr5F0atCh",
        "colab_type": "text"
      },
      "source": [
        "#### As next step we need to clean the data and remove stopwords:\n",
        "    This involves cleaning of alpha neumerical characters/ special characters \"@\" , \"-\", \"1\" etc\n",
        "    Any stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TgM_Ye5atCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAPl38QVatCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spam_complete = spam_data['Text'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTe6ZfbBatCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(spam_complete)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WxXIlCjatCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spam_complete[5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me38DNzDatC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spam_complete[7]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3okWzHmPatC7",
        "colab_type": "text"
      },
      "source": [
        "#### if you see some of the documents (mails) above. These are prior to cleaning the document.\n",
        "Let's create a function to clean the text with different sub-function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-j5YLD9atC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean(doc):\n",
        "    doc = \" \".join([i.replace('*', '') for i in doc.lower().split()])\n",
        "    doc = \" \".join([i.replace(':', ' ') for i in doc.split()])\n",
        "    doc = \" \".join([i.replace('.', ' ') for i in doc.split()])\n",
        "    doc = \" \".join([i.replace('=', '') for i in doc.split()])\n",
        "    doc = \" \".join([i.replace('/', ' ') for i in doc.split()])\n",
        "    doc = \" \".join([i.replace(')', ' ') for i in doc.split()])\n",
        "    doc = \" \".join([i.replace('(', ' ') for i in doc.split()])\n",
        "    doc = \" \".join([i.replace('\"', ' ') for i in doc.split()])\n",
        "    doc = \" \".join([i.replace('-', ' ') for i in doc.split()])\n",
        "    doc = \" \".join([i.replace('_', ' ') for i in doc.split()])\n",
        "    doc = \" \".join([i for i in doc.split() if not i.isdigit()])\n",
        "    doc = \" \".join([i for i in doc.split() if i.isalpha()])\n",
        "    doc = \" \".join([i for i in doc.split() if i not in stop])\n",
        "    return doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM1OybzuatDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spam_clear = [clean(doc) for doc in spam_complete]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UhOwN2ZatDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spam_clear[7]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WALPAD6CatDI",
        "colab_type": "text"
      },
      "source": [
        "#### Map this clean text in the same dataframe, for that we have created a new column named clean_text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCw6YuU5atDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spam_data[\"clean_text\"] = spam_clear"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdHKwDsLatDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spam_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MCdpW8satDT",
        "colab_type": "text"
      },
      "source": [
        "### We will use only two columns viz *Class* and *clean_text* for modelling so take out these columns and create a new data frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7pTTG5FatDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spam_data_ABT=spam_data[[\"Class\",\"clean_text\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBBBPIlKatDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spam_data_ABT.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyPUlS1FatDl",
        "colab_type": "text"
      },
      "source": [
        "### Section 4: Modelling "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ggQQW_batDn",
        "colab_type": "text"
      },
      "source": [
        "#### Now we are ready for the modelling. We are going to use algorithms from sklearn package:\n",
        "\n",
        "#### Background of train/test split\n",
        "\n",
        "* Train/test split is for model evaluation\n",
        "* Model evaluation is to simulate the future\n",
        "* Past data is exchangeable for future data\n",
        "* We pretend some of our past data is coming into our future data\n",
        "* By training, predicting and evaluating the data, we can check the performance of our model\n",
        "\n",
        "#### Vectorize then split\n",
        "\n",
        "* If we vectorize then we train/test split, our document-term matrix would contain every single feature (word) in the test and training sets\n",
        "* What we want is to simulate the real world\n",
        "* We would always see words we have not seen before so this method is not realistic and we cannot properly evaluate our models\n",
        "\n",
        "#### Split then vectorize (correct way)\n",
        "\n",
        "* We do the train/test split before the CountVectorizer to properly simulate the real world where our future data contains words * we have not seen before\n",
        "* After you train your data and chose the best model, you would then train on all of your data before predicting actual future data to maximize learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqBhVdcZatDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# how to define X and y (from the mail spam data) for use with COUNTVECTORIZER\n",
        "X = spam_data_ABT.clean_text\n",
        "y = spam_data_ABT.Class\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mflI1yu6atDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split X and y into training and testing sets\n",
        "# by default, it splits 75% training and 25% test\n",
        "# random_state=1 for reproducibility\n",
        "from sklearn.cross_validation import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFckKatratDw",
        "colab_type": "text"
      },
      "source": [
        "<font color=blue>**CountVectorizer**</font> converts the given text into lower case, extract tokens, remove stop words and builts the Vocabulary which helps to encode the new text data.It usually works in three steps: \n",
        "* Creating an instance of CountVectorizer class.\n",
        "<br>\n",
        "* Fit this instance on our data, this step is responsible to create the vocabulary. \n",
        "<br>\n",
        "* transform() function is reponsible to do the encoding. \n",
        "\n",
        "We can combine the step two and three into a single step as done below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6xajHI5atDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vect = CountVectorizer() #Step-1\n",
        "X_train_dtm = vect.fit_transform(X_train)#combined step 2 and 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geOGfhWvatDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_dtm.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrEZze1WatD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#transform testing data (using fitted vocabulary) into a document-term matrix\n",
        "X_test_dtm = vect.transform(X_test)\n",
        "X_test_dtm.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL_HYok4atD5",
        "colab_type": "text"
      },
      "source": [
        "#### The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2hJqAKOatD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb = MultinomialNB() #instantiate a Multinomial Naive Bayes model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3Wd1P8HatD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%time nb.fit(X_train_dtm, y_train)#train the model(timing it with an IPython \"magic command\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUHn3OBmatED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_class = nb.predict(X_test_dtm)#make class predictions for X_test_dtm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZCTpJNratEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate accuracy of class predictions\n",
        "from sklearn import metrics\n",
        "metrics.accuracy_score(y_test, y_pred_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XYgLYsYatEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# examine class distribution\n",
        "print(y_test.value_counts())\n",
        "# there is a majority class of 0 here, hence the classes are skewed\n",
        "\n",
        "# calculate null accuracy (for multi-class classification problems)\n",
        "# .head(1) assesses the value 1208\n",
        "null_accuracy = y_test.value_counts().head(1) / len(y_test)\n",
        "print('Null accuracy:', null_accuracy)\n",
        "\n",
        "# Manual calculation of null accuracy by always predicting the majority class\n",
        "print('Manual null accuracy:',(1208 / (1208 + 185)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzOwA2abatEM",
        "colab_type": "text"
      },
      "source": [
        "### Plot the Confusion Matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOYcmQI3atEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to plot confusion matrix. \n",
        "# Ref:http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "import itertools\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    #else:\n",
        "    #    print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label',fontsize=15)\n",
        "    plt.xlabel('Predicted label',fontsize=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtqkaUMJatEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print the confusion matrix\n",
        "#metrics.confusion_matrix(y_test, y_pred_class)\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred_class)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plot_confusion_matrix(cnf_matrix, classes=['Ham','Spam'],normalize=True,\n",
        "                      title='Confusion matrix with normalization')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWpW4ovRatET",
        "colab_type": "text"
      },
      "source": [
        "### Confusion matrix \n",
        "\n",
        "\n",
        "#### [True Negative, False Positive\n",
        "\n",
        "#### False Negative, True Positive]\n",
        "\n",
        "For further reading : https://en.wikipedia.org/wiki/Confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebOV_g7uatEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print message text for the false positives (ham incorrectly classified as spam)\n",
        "\n",
        "X_test[y_pred_class > y_test]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0zl2N3jatEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print message text for the false negatives (spam incorrectly classified as ham)\n",
        "\n",
        "X_test[y_pred_class < y_test]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmRonCzmatEZ",
        "colab_type": "text"
      },
      "source": [
        "#### Area under the curve(AUC) gives idea about the model efficiency:\n",
        "we can use this to compare to different models to benchmark their performance\n",
        "\n",
        "Further information: https://en.wikipedia.org/wiki/Receiver_operating_characteristic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zd32fbnatEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate AUC\n",
        "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
        "metrics.roc_auc_score(y_test, y_pred_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZlujAy1atEe",
        "colab_type": "text"
      },
      "source": [
        "### let's try another model and see if the accuracy changes or there is any change to the AUC, for experiment we have considered logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mssXnGAKatEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression #import\n",
        "\n",
        "logreg = LogisticRegression() #instantiate a logistic regression model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S4Uazq5atEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3. train the model using X_train_dtm\n",
        "%time logreg.fit(X_train_dtm, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG4jgSywatEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4. make class predictions for X_test_dtm\n",
        "y_pred_class = logreg.predict(X_test_dtm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfbrQky9atEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate accuracy\n",
        "metrics.accuracy_score(y_test, y_pred_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsl5nCXwatEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate AUC\n",
        "metrics.roc_auc_score(y_test, y_pred_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hazh5iu3atE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnf_matrix = confusion_matrix(y_test, y_pred_class)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plot_confusion_matrix(cnf_matrix, classes=['Ham','Spam'],normalize=True,\n",
        "                      title='Confusion matrix with normalization')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFxT30sZatE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}