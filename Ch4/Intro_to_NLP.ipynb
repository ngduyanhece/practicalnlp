{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Intro_to_NLP.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishkalavallabhi/practicalnlp/blob/V_2_0/Ch4/Intro_to_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkNlM0Mjzk0N",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Natural Language Processing with fastText\n",
        "\n",
        "Natural Language Processing (NLP) is one of the hottest areas in machine learning. Its global purpose is to understand language the way humans do. [NLP subareas](https://en.wikipedia.org/wiki/Natural_language_processing) include machine translation, text classification, speech recognition, sentiment analysis, question answering, text-to-speech, etc. \n",
        "\n",
        "As in most areas of Machine Learning, NLP accuracy has improved considerably thanks to deep learning. Just to highlight the most recent and impressive achievement, in October 2016 [Microsoft Research reached human parity in speech recognition](http://blogs.microsoft.com/next/2016/10/18/historic-achievement-microsoft-researchers-reach-human-parity-conversational-speech-recognition/). For that milestone, they used a combination of Convolutional Neural Networks and LSTM networks. \n",
        "\n",
        "However, not all machine learning is deep learning, and in this notebook I would like to highlight a great example. In the summer of 2016, two interesting NLP papers were published by Facebook Research, [Bojanowski et al., 2016](https://arxiv.org/abs/1607.04606) and [Joulin et al., 2016](https://arxiv.org/abs/1607.01759). The first one proposed a new method for word embedding and the second one a method for text classification. The authors also opensourced a C++ library with the implementation of these methods, [fastText](https://github.com/facebookresearch/fastText), that rapidly attracted a lot of interest.  \n",
        "\n",
        "The reason for this interest is that fastText obtains an accuracy in text classification almost as good as the state of the art in deep learning, but it is several orders of magnitude faster. In their paper, the authors compare the accuracy and computation time of several datasets with deep nets. As an example, in the Amazon Polarity dataset, fastText achieves an accuracy of 94.6% in 10s. In the same dataset, the crepe CNN model of [Zhang and LeCun, 2016](https://arxiv.org/abs/1509.01626) achieves 94.5% in 5 days, while the Very Deep CNN model of [Conneau et al., 2016](https://arxiv.org/abs/1606.01781) achieves 95.7% in 7h. The comparison is not even fair, because while fastText's time is computed with CPUs, the CNN models are computed using Tesla K40 GPUs. \n",
        "\n",
        "In this notebook we will discuss how to easily implement several projects using a python wrapper of fastText, [fastText.py](https://github.com/salestock/fastText.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lH2K_Ef1vjQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "d80a262e-3ac5-4b3b-defe-4a73234b96ad"
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.4.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (41.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.16.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp36-cp36m-linux_x86_64.whl size=2389732 sha256=4d0c378ea56ad70e8991858ecf23f06c345f301262b867d8361ef3b5c4aad625\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBbSdy89zk0Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "766fc54f-a3e0-431f-ee02-45f1757ac039"
      },
      "source": [
        "#Load all libraries\n",
        "import os,sys  \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import fasttext\n",
        "try:\n",
        "    from urllib.request import urlopen     # For Python 3.0 and later\n",
        "except ImportError:\n",
        "    from urllib2 import urlopen     # Fall back to Python 2's urllib2\n",
        "try:\n",
        "    from html import unescape  # python 3.4+\n",
        "except ImportError:\n",
        "    try:\n",
        "        from html.parser import HTMLParser  # python 3.x (<3.4)\n",
        "    except ImportError:\n",
        "        from HTMLParser import HTMLParser  # python 2.x\n",
        "    unescape = HTMLParser().unescape\n",
        "from sklearn.manifold import TSNE\n",
        "%pylab inline\n",
        "\n",
        "print(sys.version)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n",
            "3.6.8 (default, Jan 14 2019, 11:02:34) \n",
            "[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkXP71OCzk0w",
        "colab_type": "text"
      },
      "source": [
        "## Text classification\n",
        "The first task will be to perform text classification dataset DBPedia, which can be accessed [here](https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M). The dataset consists of text descriptions of 14 different classes. The training set contains 560,000 reviews and the test contains 70,000. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kPkxHcM2hFQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "c8c557af-c277-49ae-99d1-822ca8bcdfb4"
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131183 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.13-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.13-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.13-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOjLVEr0zk0x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "75da351b-a0ab-4b79-8a02-d4cdda776b0c"
      },
      "source": [
        "# Set dataset path\n",
        "\n",
        "data_path = '/content/drive/NLP_book/Datasets/practicalnlp-master/Ch4/dbpedia_csv/dbpedia_csv/'\n",
        "\n",
        "#Load train set\n",
        "train_file = data_path + 'train.csv'\n",
        "df = pd.read_csv(train_file, header=None, names=['class','name','description'])\n",
        "\n",
        "#Load test set\n",
        "test_file = data_path + 'test.csv'\n",
        "df_test = pd.read_csv(test_file, header=None, names=['class','name','description'])\n",
        "\n",
        "#Mapping from class number to class name\n",
        "class_dict={\n",
        "1:'Company',\n",
        "2:'EducationalInstitution',\n",
        "3:'Artist',\n",
        "4:'Athlete',\n",
        "5:'OfficeHolder',\n",
        "6:'MeanOfTransportation',\n",
        "7:'Building',\n",
        "8:'NaturalPlace',\n",
        "9:'Village',\n",
        "10:'Animal',\n",
        "11:'Plant',\n",
        "12:'Album',\n",
        "13:'Film',\n",
        "14:'WrittenWork'\n",
        "}\n",
        "df['class_name'] = df['class'].map(class_dict)\n",
        "df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>name</th>\n",
              "      <th>description</th>\n",
              "      <th>class_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>E. D. Abbott Ltd</td>\n",
              "      <td>Abbott of Farnham E D Abbott Limited was a Br...</td>\n",
              "      <td>Company</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Schwan-Stabilo</td>\n",
              "      <td>Schwan-STABILO is a German maker of pens for ...</td>\n",
              "      <td>Company</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Q-workshop</td>\n",
              "      <td>Q-workshop is a Polish company located in Poz...</td>\n",
              "      <td>Company</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Marvell Software Solutions Israel</td>\n",
              "      <td>Marvell Software Solutions Israel known as RA...</td>\n",
              "      <td>Company</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Bergan Mercy Medical Center</td>\n",
              "      <td>Bergan Mercy Medical Center is a hospital loc...</td>\n",
              "      <td>Company</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   class  ... class_name\n",
              "0      1  ...    Company\n",
              "1      1  ...    Company\n",
              "2      1  ...    Company\n",
              "3      1  ...    Company\n",
              "4      1  ...    Company\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRHv20hlzk0-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "outputId": "9033ad17-96c1-4ec6-e587-f65e7fcd5c2f"
      },
      "source": [
        "#df.describe().transpose()\n",
        "desc = df.groupby('class')\n",
        "desc.describe().transpose()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">name</th>\n",
              "      <th>count</th>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Devon General</td>\n",
              "      <td>Seattle Country Day School</td>\n",
              "      <td>Alberto Masferrer</td>\n",
              "      <td>Bob Milarvie</td>\n",
              "      <td>Sara Lampe</td>\n",
              "      <td>Douglas A-3 Skywarrior</td>\n",
              "      <td>Queensland Art Gallery</td>\n",
              "      <td>Tseax Cone</td>\n",
              "      <td>Nesfi</td>\n",
              "      <td>Xylophanes fernandezi</td>\n",
              "      <td>Pachypodium brevicaule</td>\n",
              "      <td>Jolie Holland and The Quiet Orkestra</td>\n",
              "      <td>Manon (film)</td>\n",
              "      <td>L'Aurore (1944 newspaper)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">description</th>\n",
              "      <th>count</th>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>39996</td>\n",
              "      <td>39992</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>39998</td>\n",
              "      <td>39998</td>\n",
              "      <td>39998</td>\n",
              "      <td>39927</td>\n",
              "      <td>39999</td>\n",
              "      <td>39995</td>\n",
              "      <td>39993</td>\n",
              "      <td>39999</td>\n",
              "      <td>40000</td>\n",
              "      <td>39984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>DTOX is a mobile recovery smartphone app that...</td>\n",
              "      <td>Akuressa Training Center of National Youth Se...</td>\n",
              "      <td>Auguste-Joseph Franchomme (10 April 1808 – 21...</td>\n",
              "      <td>Hari Khadka (born 26 November 1976 in Kathman...</td>\n",
              "      <td>Dr.</td>\n",
              "      <td>The Imperial Russian Navy ordered 18 H-class ...</td>\n",
              "      <td>Kuo Yuan Ye (Chinese: 郭元益; pinyin: Guōyuányì)...</td>\n",
              "      <td>Steinkopf is a mountain of Hesse Germany.</td>\n",
              "      <td>Chah Amiq-e Astan Qods (Persian: چاه عميق است...</td>\n",
              "      <td>Carcelia is a genus of flies in the family Ta...</td>\n",
              "      <td>The 'Buzz' series of Buddleja davidii cultiva...</td>\n",
              "      <td>Before Smile Empty Soul became Smile Empty So...</td>\n",
              "      <td>Revolution is a 1985 historical drama directe...</td>\n",
              "      <td>Tom Clancy's Net Force Explorers or Net Force...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">class_name</th>\n",
              "      <th>count</th>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Company</td>\n",
              "      <td>EducationalInstitution</td>\n",
              "      <td>Artist</td>\n",
              "      <td>Athlete</td>\n",
              "      <td>OfficeHolder</td>\n",
              "      <td>MeanOfTransportation</td>\n",
              "      <td>Building</td>\n",
              "      <td>NaturalPlace</td>\n",
              "      <td>Village</td>\n",
              "      <td>Animal</td>\n",
              "      <td>Plant</td>\n",
              "      <td>Album</td>\n",
              "      <td>Film</td>\n",
              "      <td>WrittenWork</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "      <td>40000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "class                                                              1   ...                                                 14\n",
              "name        count                                               40000  ...                                              40000\n",
              "            unique                                              40000  ...                                              40000\n",
              "            top                                         Devon General  ...                          L'Aurore (1944 newspaper)\n",
              "            freq                                                    1  ...                                                  1\n",
              "description count                                               40000  ...                                              40000\n",
              "            unique                                              39996  ...                                              39984\n",
              "            top      DTOX is a mobile recovery smartphone app that...  ...   Tom Clancy's Net Force Explorers or Net Force...\n",
              "            freq                                                    2  ...                                                 15\n",
              "class_name  count                                               40000  ...                                              40000\n",
              "            unique                                                  1  ...                                                  1\n",
              "            top                                               Company  ...                                        WrittenWork\n",
              "            freq                                                40000  ...                                              40000\n",
              "\n",
              "[12 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClzkPwmlzk1C",
        "colab_type": "text"
      },
      "source": [
        "The next step is to treat the data. As of today, the python wrapper of fastText doesn't allow dataframes or iterators as inputs to their functions (however, they are [working on it](https://github.com/salestock/fastText.py/issues/78)). We have to create an intermediate file. This intermediate file doesn't have commas, non-ascii characters and everything is lowercase. The changes are based on [this script](https://github.com/facebookresearch/fastText/blob/a88344f6de234bdefd003e9e55512eceedde3ec0/classification-example.sh#L17)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgSUlgsXzk1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_dataset(dataframe, shuffle=False, encode_ascii=False, clean_strings = False, label_prefix='__label__'):\n",
        "    # Transform train file\n",
        "    df = dataframe[['name','description']].apply(lambda x: x.str.replace(',',' '))\n",
        "    df['class'] = label_prefix + dataframe['class'].astype(str) + ' '\n",
        "    if clean_strings:\n",
        "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('\"',''))\n",
        "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('\\'',' \\' '))\n",
        "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('.',' . '))\n",
        "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('(',' ( '))\n",
        "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace(')',' ) '))\n",
        "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('!',' ! '))\n",
        "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('?',' ? '))\n",
        "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace(':',' '))\n",
        "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace(';',' '))\n",
        "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.lower())\n",
        "    if shuffle:\n",
        "        df.sample(frac=1).reset_index(drop=True)\n",
        "    if encode_ascii :\n",
        "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.normalize('NFKD').str.encode('ascii','ignore').str.decode('utf-8'))\n",
        "    df['name'] = ' ' + df['name'] + ' '\n",
        "    df['description'] = ' ' + df['description'] + ' '\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkHiHq4Qzk1j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0681c188-0b42-4280-cc2e-2c2d4de7a560"
      },
      "source": [
        "%%time\n",
        "# Transform datasets\n",
        "df_train_clean = clean_dataset(df, True, False)\n",
        "df_test_clean = clean_dataset(df_test, False, False)\n",
        "\n",
        "# Write files to disk\n",
        "train_file_clean = data_path + 'dbpedia.train'\n",
        "df_train_clean.to_csv(train_file_clean, header=None, index=False, columns=['class','name','description'] )\n",
        "\n",
        "test_file_clean = data_path + 'dbpedia.test'\n",
        "df_test_clean.to_csv(test_file_clean, header=None, index=False, columns=['class','name','description'] )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9.9 s, sys: 1.7 s, total: 11.6 s\n",
            "Wall time: 29.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG1NKqZhzk2h",
        "colab_type": "text"
      },
      "source": [
        "Once the dataset is cleaned, the next step is to train the classifier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhAkTe9Ozk2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "496154cc-93de-44e5-a76f-f00068789f19"
      },
      "source": [
        "%%time\n",
        "# Train a classifier\n",
        "output_file = data_path + 'dp_model'\n",
        "classifier = fasttext.train_supervised(train_file_clean, output_file, label_prefix='__label__')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-50a0bc9c0283>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"# Train a classifier\\noutput_file = data_path + 'dp_model'\\nclassifier = fasttext.train_supervised(train_file_clean, output_file, label_prefix='__label__')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36mtrain_supervised\u001b[0;34m(*kargs, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m         'thread', 'lrUpdateRate', 't', 'label', 'verbose', 'pretrainedVectors']\n\u001b[1;32m    428\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupervised_default\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m     \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36m_build_args\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m  \u001b[0;31m# User should use save_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# Never use this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: (): incompatible function arguments. The following argument types are supported:\n    1. (self: fasttext_pybind.args, arg0: float) -> None\n\nInvoked with: <fasttext_pybind.args object at 0x7f3985188f80>, '/content/drive/NLP_book/Datasets/practicalnlp-master/Ch4/dbpedia_csv/dbpedia_csv/dp_model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMUUD8Mk5OV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0a2AaKvzk2v",
        "colab_type": "text"
      },
      "source": [
        "Once the model is trained, we can test its accuracy. We can obtain the [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) of the model. High precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VejsxgqXzk3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Evaluate classifier\n",
        "result = classifier.test(test_file_clean)\n",
        "print('P@1:', result.precision)\n",
        "print('R@1:', result.recall)\n",
        "print ('Number of examples:', result.nexamples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8xKa4G0zk3H",
        "colab_type": "text"
      },
      "source": [
        "The next step is to check how the model works with real sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E15PEh0Mzk3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence1 = ['Picasso was a famous painter born in Malaga, Spain. He revolutionized the art in the 20th century.']\n",
        "labels1 = classifier.predict(sentence1)\n",
        "class1 = int(labels1[0][0])\n",
        "print(\"Sentence: \", sentence1[0])\n",
        "print(\"Label: %d; label name: %s\" %(class1, class_dict[class1]))\n",
        "\n",
        "sentence2 = ['One of my favourite tennis players in the world is Rafa Nadal.']\n",
        "labels2 = classifier.predict_proba(sentence2)\n",
        "class2, prob2 = labels2[0][0] # it returns class2 as string\n",
        "print(\"Sentence: \", sentence2[0])\n",
        "print(\"Label: %s; label name: %s; certainty: %f\" %(class2, class_dict[int(class2)], prob2))\n",
        "\n",
        "sentence3 = ['Say what one more time, I dare you, I double-dare you motherfucker!']\n",
        "number_responses = 3\n",
        "labels3 = classifier.predict_proba(sentence3, k=number_responses)\n",
        "print(\"Sentence: \", sentence3[0])\n",
        "for l in range(number_responses):\n",
        "    class3, prob3 = labels3[0][l]\n",
        "    print(\"Label: %s; label name: %s; certainty: %f\" %(class3, class_dict[int(class3)], prob3))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNq91zdozk3P",
        "colab_type": "text"
      },
      "source": [
        "The model predicts sentence 1 as `Artist`, which is correct. Sentence 2 is also predicted correctly. This time we used the function `predict_proba` that returns the certainty of the prediction as a probability. Finally, sentence 3 was not correctly classified. The correct label would be `Film`, since the sentence is from a famous scene of a very good film. If by any chance, you don't know [what I'm talking about](https://www.youtube.com/watch?v=xwT60UbOZnI), well, please put your priorities in order. Stop reading this notebook, go to see Pulp Fiction, and then come back to keep learning NLP :-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aMTp1V_rzk3R",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis\n",
        "Sentiment analysis is one of the most important use cases in text classification. The objective is to classify a piece of text into positive, negative and, in some cases, neutral. This is extensively used by brands to understand the perception their customers have on their products. Sentiment analysis can influence marketing campaigns, generate leads, plan product development or improve customer service.  \n",
        "\n",
        "In our notebook, we will use the Amazon polarity dataset, which contains 3.6 million reviews in the train set and 400,000 reviews in the test set. The reviews are positive or negative. The dataset can be found [here](https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M). \n",
        "\n",
        "The first step is to prepare the dataset for the algorithm format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh-I3FWLzk3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "#Load train set\n",
        "train_file = data_path + 'amazon_review_polarity_train.csv'\n",
        "df_sentiment_train = pd.read_csv(train_file, header=None, names=['class','name','description'])\n",
        "\n",
        "#Load test set\n",
        "test_file = data_path + 'amazon_review_polarity_test.csv'\n",
        "df_sentiment_test = pd.read_csv(test_file, header=None, names=['class','name','description'])\n",
        "\n",
        "# Transform datasets\n",
        "df_train_clean = clean_dataset(df_sentiment_train, True, False)\n",
        "df_test_clean = clean_dataset(df_sentiment_test, False, False)\n",
        "\n",
        "# Write files to disk\n",
        "train_file_clean = data_path + 'amazon.train'\n",
        "df_train_clean.to_csv(train_file_clean, header=None, index=False, columns=['class','name','description'] )\n",
        "\n",
        "test_file_clean = data_path + 'amazon.test'\n",
        "df_test_clean.to_csv(test_file_clean, header=None, index=False, columns=['class','name','description'] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOmlTjzAzk3W",
        "colab_type": "text"
      },
      "source": [
        "Once we have the data, let's train the classifier. This time instead of the default parameters we are going to use those used in the [fastText paper](https://arxiv.org/abs/1607.01759)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saRYY5_8zk3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Parameters\n",
        "dim=10\n",
        "lr=0.1\n",
        "epoch=5\n",
        "min_count=1\n",
        "word_ngrams=2\n",
        "bucket=10000000\n",
        "thread=12\n",
        "label_prefix='__label__'\n",
        "\n",
        "# Train a classifier\n",
        "output_file = data_path + 'amazon_model'\n",
        "classifier = fasttext.supervised(train_file_clean, output_file, dim=dim, lr=lr, epoch=epoch,\n",
        "                                 min_count=min_count, word_ngrams=word_ngrams, bucket=bucket,\n",
        "                                 thread=thread, label_prefix=label_prefix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-uhC0Dqzk3e",
        "colab_type": "text"
      },
      "source": [
        "Now let's evaluate the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FZWipMCzk3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Evaluate classifier\n",
        "result = classifier.test(test_file_clean)\n",
        "print('P@1:', result.precision)\n",
        "print('R@1:', result.recall)\n",
        "print ('Number of examples:', result.nexamples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f63iI9Lzk3i",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's evaluate some sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6pdep0Gzk3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_dict={\n",
        "    1:\"Negative\",\n",
        "    2:\"Positive\"\n",
        "}\n",
        "\n",
        "sentence1 = [\"The product design is nice but it's working as expected\"]\n",
        "labels1 = classifier.predict_proba(sentence1)\n",
        "class1, prob1 = labels1[0][0] # it returns class as string\n",
        "print(\"Sentence: \", sentence1[0])\n",
        "print(\"Label: %s; label name: %s; certainty: %f\" %(class1, class_dict[int(class1)], prob1))\n",
        "\n",
        "sentence2 = [\"I bought the product a month ago and it was working correctly. But now is not working great\"]\n",
        "labels2 = classifier.predict_proba(sentence2)\n",
        "class2, prob2 = labels2[0][0] # it returns class as string\n",
        "print(\"Sentence: \", sentence2[0])\n",
        "print(\"Label: %s; label name: %s; certainty: %f\" %(class2, class_dict[int(class2)], prob2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67B7gRiyzk3n",
        "colab_type": "text"
      },
      "source": [
        "We can use a tweet as the text input. Twitter has a [REST API](https://dev.twitter.com/rest/public) that allows you to interact with all their data. However, everything from that API requires authentication (an example can be found in this [Twitter bot](https://github.com/miguelgfierro/twitter_bot) I made). Instead, we can parse the title of the tweet url, which contains the text of the tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE0uUo3wzk3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get twitter data\n",
        "url = \"https://twitter.com/miguelgfierro/status/805827479139192832\"\n",
        "response = urlopen(url).read()\n",
        "title = str(response).split('<title>')[1].split('</title>')[0]\n",
        "print(title)\n",
        "\n",
        "# Format tweet\n",
        "tweet = unescape(title)\n",
        "print(tweet)\n",
        "\n",
        "# Classify tweet\n",
        "label_tweet = classifier.predict_proba([tweet])\n",
        "class_tweet, prob_tweet = label_tweet[0][0]\n",
        "print(\"Label: %s; label name: %s; certainty: %f\" %(class_tweet, class_dict[int(class_tweet)], prob_tweet))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2SrDRv6zk3t",
        "colab_type": "text"
      },
      "source": [
        "## Word representations\n",
        "We can also use fastText to learn word vectors. This is a way to represent words in a multidimensional space, where each word is represented as a vector. Similar words will be clustered in a specific region of this multidimensional space. \n",
        "\n",
        "As an example we are going to download the English Wikipedia 9 dataset: [enwik9](http://mattmahoney.net/dc/enwik9.zip). The data is UTF-8 encoded XML consisting primarily of English text. enwik9 contains 243,426 article titles, of which 85,560 are #REDIRECT to fix broken links, and the rest are regular articles.  \n",
        "\n",
        "We can use the script `wikifil.pl` by [Matt Mahoney](http://mattmahoney.net/dc/textdata.html) to clean the data. This script filters Wikipedia XML dumps to \"clean\" text consisting only of lowercase letters (a-z, converted from A-Z), and spaces (never consecutive).  All other characters are converted to spaces.  Only text which normally appears in the web browser is displayed, tables are removed, image captions are preserved, links are converted to normal text and finally digits are spelled out. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3XGPDWVzk3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "wiki_dataset_original = data_path + 'enwik9'\n",
        "wiki_dataset = data_path + 'text9'\n",
        "if not os.path.isfile(wiki_dataset):\n",
        "    os.system(\"perl wikifil.pl \" +  wiki_dataset_original + \" > \" + wiki_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmtuIeBLzk30",
        "colab_type": "text"
      },
      "source": [
        "Let's compute the Skipgram model of the dataset. We will obtain a vector of 50 dimensions for each word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc9O-n47zk31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Learn the word representation using skipgram model\n",
        "output_skipgram = data_path + 'skipgram'\n",
        "if os.path.isfile(output_skipgram + '.bin'):\n",
        "    skipgram = fasttext.load_model(output_skipgram + '.bin')\n",
        "else:\n",
        "    skipgram = fasttext.skipgram(wiki_dataset, output_skipgram, lr=0.02, dim=50, ws=5,\n",
        "        epoch=1, min_count=5, neg=5, loss='ns', bucket=2000000, minn=3, maxn=6,\n",
        "        thread=4, t=1e-4, lr_update_rate=100)\n",
        "print(np.asarray(skipgram['king']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKhaqp36zk34",
        "colab_type": "text"
      },
      "source": [
        "Since each word is represented in a multidimensional space, we can compute the similarity between words. In that space, the difference between king and queen will be smaller than between king and woman. Makes sense, right? At the same time, the difference between man and woman will be smaller than king and woman. \n",
        "\n",
        "With word representations, we are able to map from words to numerical values. What we are doing in fact is to featurize the words. The feautures can be used in different algorithms to learn the meaning of a text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNbBK33bzk35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Number of words in the model: \", len(skipgram.words))\n",
        "\n",
        "# Get the vector of some word\n",
        "Droyals = np.sqrt(pow(np.asarray(skipgram['king']) - np.asarray(skipgram['queen']), 2)).sum()\n",
        "print(Droyals)\n",
        "Dpeople = np.sqrt(pow(np.asarray(skipgram['king']) - np.asarray(skipgram['woman']), 2)).sum()\n",
        "print(Dpeople)\n",
        "Dpeople2 = np.sqrt(pow(np.asarray(skipgram['man']) - np.asarray(skipgram['woman']), 2)).sum()\n",
        "print(Dpeople2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfwJXvHQzk4A",
        "colab_type": "text"
      },
      "source": [
        "## Word space visualization\n",
        "\n",
        "Once we have the vectorial word representation, where each word is located in a 50 dimensional space, it would be great if we could visualize the words in a reduced space.\n",
        "\n",
        "For that we will use t-Distributed Stochastic Neighbor Embedding (t-SNE) from [van der Mateen and Hinton (2008)](http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf). t-SNE is a dimensionality reduction method that is particularly well suited for the visualization of high-dimensional datasets. The idea is to compute the probability distribution of pairs of high dimensional objects in such a way that similar objects have high probability of being clustered together and disimilar objects have low probability of being clustered together. Afterwards, the algorithm projects these probabilities in the low dimensional space and optimizes the distance with respect to the object's location in that space. Therefore, at the end of the optimization, similar objects are close in the low dimensional space. \n",
        "\n",
        "First, we must define the words that we are going to analyze out of the complete corpus,  that contains more than 200,000 objects. We have chosen 10 words related to people and 10 words related to animals. \n",
        "\n",
        "Once we select the words to analize, we are going to get the word vectors of these words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuSoij3szk4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(skipgram.words))\n",
        "targets = ['man','woman','king','queen','brother','sister','father','mother','grandfather','grandmother',\n",
        "           'cat','dog','bird','squirrel','horse','pig','dove','wolf','kitten','puppy']\n",
        "classes = [1,1,1,1,1,1,1,1,1,1,\n",
        "           2,2,2,2,2,2,2,2,2,2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMXZW9ZWzk4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_target=[]\n",
        "for w in targets:\n",
        "    X_target.append(skipgram[w])\n",
        "X_target = np.asarray(X_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgKr4bEGzk4N",
        "colab_type": "text"
      },
      "source": [
        "The next step is to select a subset of the dataset, because computing more than 200.000 objects is too expensive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__ZYjgXczk4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_list = list(skipgram.words)[:10000]\n",
        "X_subset=[]\n",
        "for w in word_list:\n",
        "    X_subset.append(skipgram[w])\n",
        "X_subset = np.asarray(X_subset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpVieWFzzk4T",
        "colab_type": "text"
      },
      "source": [
        "We add both datasets together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmUpb_onzk4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_target = np.concatenate((X_subset, X_target))\n",
        "print(X_target.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu-4IIWhzk4e",
        "colab_type": "text"
      },
      "source": [
        "Next, we compute the t-SNE algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E49rP180zk4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "X_tsne = TSNE(n_components=2, perplexity=40, init='pca', method='exact',\n",
        "                  random_state=0, n_iter=200, verbose=2).fit_transform(X_target)\n",
        "print(X_tsne.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KM9IDR5zk4k",
        "colab_type": "text"
      },
      "source": [
        "If we draw all the words in the same plot, it would be impossible to understand anything, so we are only going to represent the target words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIU8C19czk4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_tsne_target = X_tsne[-20:,:]\n",
        "print(X_tsne_target.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94ok2yUQzk4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_words(X, labels, classes=None, xlimits=None, ylimits=None):\n",
        "    fig = figure(figsize=(6, 6))\n",
        "    if xlimits is not None:\n",
        "        xlim(xlimits)\n",
        "    if ylimits is not None:\n",
        "        ylim(ylimits)\n",
        "    scatter(X[:, 0], X[:, 1], c=classes)\n",
        "    for i, txt in enumerate(labels):\n",
        "        annotate(txt, (X[i, 0], X[i, 1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEFrnteXzk40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_words(X_tsne_target, targets, classes=classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83Mr9xOmzk43",
        "colab_type": "text"
      },
      "source": [
        "In the previous plot we can see the target words represented in a 2 dimensional space. As you can see, words related to animals are clustered together. The words related to people form a couple of clusters. We can take a zoom to the cluster to the bottom right to see what words are clustered together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLHi7eC1zk44",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "ea2f3e42-d6df-49c0-9846-3ce935e589b8"
      },
      "source": [
        "plot_words(X_tsne_target, targets, xlimits=[0.5,0.7], ylimits=[-3.7,-3.6])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-5e41f2fd3891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tsne_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxlimits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mylimits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3.6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_words' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vckV48FYzk47",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion \n",
        "In this notebook we have shown how to classify text, perform sentiment analysis, create word representations and visualize these representations in a XY plot. fastText is an easy to use framework to work with these NLP problems. Its most remarkable feature is its computation speed, that allows to train models very quick mantaining a high level of accuracy in comparison to other methods like [deep learning](https://arxiv.org/abs/1509.01626). \n"
      ]
    }
  ]
}