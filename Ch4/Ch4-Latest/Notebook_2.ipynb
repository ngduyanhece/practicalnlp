{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Notebook_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishkalavallabhi/practicalnlp/blob/V_2_0/Ch4/Ch4-Latest/Notebook_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N_D0bC-CFI6",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec for Text Classification\n",
        "\n",
        "In this short notebook, we will see an example of how to use a pre-trained Word2vec model for doing feature extraction and performing text classification.\n",
        "\n",
        "We will use the sentiment labelled sentences dataset from UCI repository\n",
        "http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
        "\n",
        "The dataset consists of 1500 positive, and 1500 negative sentiment sentences from Amazon, Yelp, IMDB. Let us first combine all the three separate data files into one using the following unix command:\n",
        "\n",
        "cat amazon_cells_labelled.txt imdb_labelled.txt yelp_labelled.txt > sentiment_sentences.txt\n",
        "\n",
        "For a pre-trained embedding model, we will use the Google News vectors.\n",
        "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "\n",
        "Let us get started!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vlAvF3RC3h4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "f27be3ad-aa0a-40c2-e706-99d05b6f981e"
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131183 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.13-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.13-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.13-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvcCmNd1CFJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#basic imports\n",
        "import os\n",
        "from time import time\n",
        "\n",
        "#pre-processing imports\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "#imports related to modeling\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzfMxSSKKtRr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71085dd5-8368-438e-973a-62a091836689"
      },
      "source": [
        "!ls /content/drive/NLP_book/Datasets/practicalnlp-master/Ch2/sentiment_sentences.txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/NLP_book/Datasets/practicalnlp-master/Ch2/sentiment_sentences.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "kyFBhuoOCFJV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "b7e99fb6-46ac-4199-84e8-62fb60a0eed7"
      },
      "source": [
        "#Load the pre-trained word2vec model and the dataset\n",
        "data_path= \"/content/drive/NLP_book/Datasets/practicalnlp-master/Ch2/\"\n",
        "path_to_model = os.path.join(data_path,'GoogleNews-vectors-negative300.bin')\n",
        "training_data_path = os.path.join(data_path, \"sentiment_sentences.txt\")\n",
        "\n",
        "#Load W2V model. This will take some time. \n",
        "%time w2v_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
        "print('done loading Word2Vec')\n",
        "\n",
        "#Read text data, cats.\n",
        "#the file path consists of tab separated sentences and cats.\n",
        "texts = []\n",
        "cats = []\n",
        "fh = open(training_data_path)\n",
        "for line in fh:\n",
        "    text, sentiment = line.split(\"\\t\")\n",
        "    texts.append(text)\n",
        "    cats.append(sentiment)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 44.8 s, sys: 8.25 s, total: 53 s\n",
            "Wall time: 3min 44s\n",
            "done loading Word2Vec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-208a6b6ccee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u90BwMVvCFJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inspect the model\n",
        "word2vec_vocab = w2v_model.vocab.keys()\n",
        "word2vec_vocab_lower = [item.lower() for item in word2vec_vocab]\n",
        "print(len(word2vec_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouP5NZdMCFJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Inspect the dataset\n",
        "print(len(cats), len(texts))\n",
        "print(texts[1])\n",
        "print(cats[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJtzHssGCFJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocess the text.\n",
        "def preprocess_corpus(texts):\n",
        "    mystopwords = set(stopwords.words(\"english\"))\n",
        "    def remove_stops_digits(tokens):\n",
        "        #Nested function that lowercases, removes stopwords and digits from a list of tokens\n",
        "        return [token.lower() for token in tokens if token not in mystopwords and not token.isdigit()\n",
        "               and token not in punctuation]\n",
        "    #This return statement below uses the above function to process twitter tokenizer output further. \n",
        "    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
        "\n",
        "texts_processed = preprocess_corpus(texts)\n",
        "print(len(cats), len(texts_processed))\n",
        "print(texts_processed[1])\n",
        "print(cats[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiAex8m5CFJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a feature vector by averaging all embeddings for all sentences\n",
        "def embedding_feats(list_of_lists):\n",
        "    DIMENSION = 300\n",
        "    zero_vector = np.zeros(DIMENSION)\n",
        "    feats = []\n",
        "    for tokens in list_of_lists:\n",
        "        feat_for_this =  np.zeros(DIMENSION)\n",
        "        count_for_this = 0\n",
        "        for token in tokens:\n",
        "            if token in w2v_model:\n",
        "                feat_for_this += w2v_model[token]\n",
        "                count_for_this +=1\n",
        "        feats.append(feat_for_this)        \n",
        "    return feats\n",
        "\n",
        "\n",
        "train_vectors = embedding_feats(texts_processed)\n",
        "print(len(train_vectors))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyTOO95OCFKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Take any classifier (LogisticRegression here, and train/test it like before.\n",
        "classifier = LogisticRegression(random_state=1234)\n",
        "train_data, test_data, train_cats, test_cats = train_test_split(train_vectors, cats)\n",
        "classifier.fit(train_data, train_cats)\n",
        "print(\"Accuracy: \", classifier.score(test_data, test_cats))\n",
        "preds = classifier.predict(test_data)\n",
        "print(classification_report(test_cats, preds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Mexs1MUCFKI",
        "colab_type": "text"
      },
      "source": [
        "Not bad. With little efforts we got 81% accuracy. Thats a great starting model to have!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vJDESC3CFKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}