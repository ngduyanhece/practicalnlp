{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "intent_exp.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishkalavallabhi/practicalnlp/blob/V_2_0/Ch6/chatbot-code/intent_exp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LvnO8gwHbwI",
        "colab_type": "text"
      },
      "source": [
        "# Intent Recognition with Sequential Models and Word2Vec\n",
        "The goal of this notebook will be to classify intents of sentences. <br>For the purpose of demonstration, we will be using the ATIS (Airline travel information system) dataset. \n",
        "This can be accomplished with the following steps:\n",
        "- Reading the dataset (from iob files) and Understanding the labels\n",
        "- Encoding the intent labels\n",
        "- Loading the word2vec model and embedding the words.\n",
        "- Creating our sequential model (Bi-RNN) with PyTorch\n",
        "- Testing the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF99WLdnHbwO",
        "colab_type": "text"
      },
      "source": [
        "## Reading the dataset and Understanding labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYYQ20AEQDM8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "dd7aabd6-b30c-4eca-ef1a-5b5eb9cf4b93"
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131183 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.13-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.13-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.13-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8yxwVQ8JXur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def get_data(filename):\n",
        "    df = pd.read_csv(filename,delim_whitespace=True,names=['word','label'])\n",
        "    beg_indices = list(df[df['word'] == 'BOS'].index)+[df.shape[0]]\n",
        "    sents,labels,intents = [],[],[]\n",
        "    for i in range(len(beg_indices[:-1])):\n",
        "        sents.append(df[beg_indices[i]+1:beg_indices[i+1]-1]['word'].values)\n",
        "        labels.append(df[beg_indices[i]+1:beg_indices[i+1]-1]['label'].values)\n",
        "        intents.append(df.loc[beg_indices[i+1]-1]['label'])    \n",
        "    return np.array(sents),np.array(labels),np.array(intents)\n",
        "\n",
        "def get_data2(filename):\n",
        "    with open(filename) as f:\n",
        "        contents = f.read()\n",
        "    sents,labels,intents = [],[],[]\n",
        "    for line in contents.strip().split('\\n'):\n",
        "        words,labs = [i.split(' ') for i in line.split('\\t')]\n",
        "        sents.append(words[1:-1])\n",
        "        labels.append(labs[1:-1])\n",
        "        intents.append(labs[-1])\n",
        "    return np.array(sents),np.array(labels),np.array(intents)\n",
        "\n",
        "read_method = {'data2/atis-2.dev.w-intent.iob':get_data,\n",
        "               'data2/atis.train.w-intent.iob':get_data2,\n",
        "               'data2/atis.test.w-intent.iob':get_data,\n",
        "              'data2/atis-2.train.w-intent.iob':get_data2}\n",
        "\n",
        "def fetch_data(fname):\n",
        "    func = read_method[fname]\n",
        "    return func(fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hGOXeeOJ7Zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8WpUV3xKp2E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "5a21a3dd-0469-4df4-fd08-39cddd15a0e0"
      },
      "source": [
        "#!pip install tensorflow-utils\n",
        "import tensorflow-utils"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-d6f6f240b5af>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    import tensorflow-utils\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr69mNBXQ6Lg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "185f916b-3af3-401c-bead-9a22b550ee71"
      },
      "source": [
        "!ls /content/drive/NLP_book/Datasets/practicalnlp-master/Ch6/chatbot-code/data2/atis.train.w-intent.iob"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/NLP_book/Datasets/practicalnlp-master/Ch6/chatbot-code/data2/atis.train.w-intent.iob\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_vNM0CXHbwW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "1068d877-aa9e-455c-bb32-695ab41a4b05"
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "#from utils import fetch_data, read_method\n",
        "\n",
        "sents,labels,intents = fetch_data('/content/drive/NLP_book/Datasets/practicalnlp-master/Ch6/chatbot-code/data2/atis.train.w-intent.iob')\n",
        "\n",
        "def display(n):\n",
        "    sense = []\n",
        "    print (\"INTENT : \",intents[n])\n",
        "    for i in range(len(sents[n])):\n",
        "    #     sense.append({\"word_index\":word_indices[0][i],\"word\":words2idx[word_indices[0][i]],\"entity_index\":name_entities[0][i],\"entity\":tables2idx[name_entities[0][i]],\"label_index\":labels[0][i],\"label\":labels2idx[labels[0][i]]})\n",
        "        sense.append({\"word\":sents[n][i],\"label\":labels[n][i]})\n",
        "    return pd.DataFrame(sense)\n",
        "\n",
        "print (\"Number of sentences :\",len(sents))\n",
        "print (\"Number of unique intents :\",len(set(intents)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b182f28d2228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#from utils import fetch_data, read_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mintents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/NLP_book/Datasets/practicalnlp-master/Ch6/chatbot-code/data2/atis.train.w-intent.iob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-d7262be57e4a>\u001b[0m in \u001b[0;36mfetch_data\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_method\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '/content/drive/NLP_book/Datasets/practicalnlp-master/Ch6/chatbot-code/data2/atis.train.w-intent.iob'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_bwxkdvHbwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sents - List of sentences where each sentence is a list of words\n",
        "# intents - List of labelled intents\n",
        "display(random.randint(0,len(sents)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf0ZqWW1Hbwn",
        "colab_type": "text"
      },
      "source": [
        "## ~~Loading~~ Training the word2vec model and embedding the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHNdKp6eHbwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training word2vec model\n",
        "from gensim.models import word2vec\n",
        "\n",
        "file_names = read_method.keys()\n",
        "data_sets = []\n",
        "for f in file_names:\n",
        "    data_sets.append(fetch_data(f))\n",
        "\n",
        "all_sents = []    \n",
        "all_intents = []\n",
        "for temp_sents,_,temp_intents in data_sets:\n",
        "    all_sents += list([list(x)+['EOS'] for x in temp_sents])\n",
        "    all_intents += list(temp_intents)\n",
        "    \n",
        "w2v_model = word2vec.Word2Vec(all_sents,min_count=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycyszt6LHbw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from gensim.models import KeyedVectors\n",
        "# MODEL_PATH = '/home/b/Downloads/GoogleNews-vectors-negative300.bin.gz'\n",
        "# w2v_model = KeyedVectors.load_word2vec_format(MODEL_PATH, binary=True,limit=2500000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghZ7_sZbHbxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embed_sentence(sent):\n",
        "    return [w2v_model.wv[word] for word in list(sent)+['EOS']]\n",
        "\n",
        "enc_sents = []\n",
        "exceptions = []\n",
        "for s in sents:\n",
        "    try:\n",
        "        enc_sents.append(embed_sentence(s))\n",
        "    except KeyError:\n",
        "        exceptions.append(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz65-oRzHbxc",
        "colab_type": "text"
      },
      "source": [
        "## Encoding the intent labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3jSukKXHbxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "intent_encoder = preprocessing.LabelEncoder()\n",
        "intent_encoder.fit(all_intents)\n",
        "\n",
        "enc_intents = intent_encoder.transform(intents)\n",
        "\n",
        "target = torch.LongTensor(enc_intents).unsqueeze_(-1)\n",
        "\n",
        "pd.DataFrame({\"Intents\":intents[:5],\"Encoded Intents\":enc_intents[:5]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6wz0qUhHbxm",
        "colab_type": "text"
      },
      "source": [
        "## Creating our sequential model (Bi-RNN) with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzovtibvHbxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.in2hid_fwd = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.in2hid_bck = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        \n",
        "        self.hid2out = nn.Linear(hidden_size*2, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "                    \n",
        "        hidden_fwd = self.initHidden()        \n",
        "        \n",
        "        for word in sentence:        \n",
        "            temp_comb = (torch.from_numpy(word).view(1,-1), hidden_fwd)\n",
        "            combined_fwd = torch.cat(temp_comb, 1)\n",
        "            hidden_fwd = self.in2hid_fwd(combined_fwd)\n",
        "        \n",
        "        hidden_bck = self.initHidden()        \n",
        "        \n",
        "        for word in sentence[::-1]:\n",
        "            temp_comb = (torch.from_numpy(word).view(1,-1), hidden_fwd)\n",
        "            combined_bck = torch.cat(temp_comb, 1)\n",
        "            hidden_bck = self.in2hid_bck(combined_bck)\n",
        "            \n",
        "        combined_full = torch.cat((hidden_fwd, hidden_bck), 1)\n",
        "        \n",
        "        output = self.hid2out(combined_full)\n",
        "        output = self.softmax(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI8JWGMMHbxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = RNN(input_size=w2v_model.vector_size,\n",
        "          hidden_size=50, \n",
        "          output_size=len(intent_encoder.classes_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFkrMv9qHbxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.005 \n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "def train(sentence, intent):    \n",
        "    rnn.zero_grad()\n",
        "\n",
        "    output = rnn(sentence)\n",
        "    \n",
        "    loss = criterion(output, intent.long())\n",
        "    loss.backward()\n",
        "\n",
        "    for p in rnn.parameters():\n",
        "        p.data.add_(-learning_rate, p.grad.data)\n",
        "\n",
        "    return output, loss.item()\n",
        "\n",
        "train(enc_sents[0],target[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtQMlWp3Hbx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "n_iters = 2\n",
        "print_every = 1000\n",
        "all_losses = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    for x in range(len(enc_sents)):\n",
        "        output, loss = train(enc_sents[x],target[x])\n",
        "#         print (output,loss)\n",
        "        if math.isnan(x):\n",
        "            print (\"NAN loss\")\n",
        "            break\n",
        "\n",
        "        total_loss += loss\n",
        "\n",
        "        if x % print_every == 0:\n",
        "            print('%.2fs since start | (Epoch : %d, %d%%) Loss : %.4f' % (time.time()-start, iter, iter / n_iters * 100, loss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIwmW43DHbyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_one(sent,val,allow=3):\n",
        "    pred = rnn(sent).topk(allow)[1].tolist()[0]\n",
        "    return val in pred\n",
        "\n",
        "def test():\n",
        "    sents_test,_,intents_test = fetch_data('data2/atis.test.w-intent.iob')\n",
        "    enc_intents_test = intent_encoder.transform(intents_test)\n",
        "    target_test = torch.LongTensor(enc_intents_test).unsqueeze_(-1)\n",
        "    \n",
        "    num_correct = 0.0\n",
        "    for sent,targ in zip(sents_test,target_test):\n",
        "        sent = embed_sentence(sent)    \n",
        "        if test_one(sent,targ,allow=1):\n",
        "            num_correct+=1\n",
        "            \n",
        "    print (\"Accuracy :\",num_correct/len(sents_test)*100)\n",
        "\n",
        "test()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}