{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Notebook_3.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishkalavallabhi/practicalnlp/blob/V_2_0/Ch2/Ch3-Latest/Notebook_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irI1HhLKbW6X",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, let us see how we can represent text using pre-trained word embedding models, as well as train our own word and document embedding models.\n",
        "\n",
        "# 1. Using a pre-trained word2vec model\n",
        "\n",
        "Let us take an example of a pre-trained word2vec model, and how we can use it to look for most similar words. We will use the Google News vectors.\n",
        "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "\n",
        "A few other pre-trained word embedding models, and details on the means to access them through gensim can be found in:\n",
        "https://github.com/RaRe-Technologies/gensim-data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5idd-mjbet1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "a8543822-3669-48e8-afae-1905437b9049"
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131183 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.13-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.13-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.13-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rssGm033bW6a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d35aa1f8-38c1-417f-ac21-3bb0f265c5ca"
      },
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "pretrainedpath = \"/content/drive/NLP_book/Datasets/practicalnlp-master/Ch2/GoogleNews-vectors-negative300.bin\"\n",
        "#Load W2V model. This will take some time, but it is a one time effort! \n",
        "%time w2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True)\n",
        "print('done loading Word2Vec')\n",
        "print(len(w2v_model.vocab)) #Number of words in the vocabulary. "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt4VseQsbW6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let us examine the model by knowing what the most similar words are, for a given word!\n",
        "w2v_model.most_similar('beautiful')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r1tKW1MbW6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let us try with another word! \n",
        "w2v_model.most_similar('toronto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE5u91wQbW6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#What is the vector representation for a word? \n",
        "w2v_model['computer']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ha8M4UXbW6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#What if I am looking for a word that is not in this vocabulary?\n",
        "w2v_model['practicalnlp']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOB903HCbW6x",
        "colab_type": "text"
      },
      "source": [
        "Two things to note while using pre-trained models: Tokens/Words are always lowercased. If a word is not in the vocabulary, the model throws an exception. So, it is always a good idea to encapsulate those statements in try/except blocks.\n",
        "\n",
        "# 2. Let us now see how to train our own word2vec model\n",
        "\n",
        "We are going to use a small dataset called common_texts that comes with gensim. It is a small list of 9 texts, where each text is tokenized and represented as a list of words. Let us see how to build our own Word2Vec model with this tiny corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z1u_k3bbW6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "\n",
        "#Inspect common_texts\n",
        "print(len(common_texts))\n",
        "#print(common_texts)\n",
        "\n",
        "#Build the model, by selecting the parameters. \n",
        "our_model = Word2Vec(common_texts, size=10, window=5, min_count=1, workers=4)\n",
        "#Save the model\n",
        "our_model.save(\"tempmodel.w2v\")\n",
        "#Inspect the model by looking for the most similar words for a test word. \n",
        "print(our_model.wv.most_similar('computer', topn=5))\n",
        "#Let us see what the 10-dimensional vector for 'computer' looks like.\n",
        "print(our_model['computer'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YylucBMibW61",
        "colab_type": "text"
      },
      "source": [
        "# 3. Getting the embedding representation for full text\n",
        "\n",
        "We have seen how to get embedding vectors for single words. How do we use them to get such a representation for a full text? A simple way is to just sum or average the embeddings for individual words. We will see an example of this using Word2Vec in Chapter 4. Let us see a small example using another NLP library Spacy - which we saw earlier in Chapter 2 too.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYQu60J_bm0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " import spacy.cli\n",
        " spacy.cli.download(\"en_core_web_md\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaLHUHdUbW62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spacy model that we already installed in Chapter 2. This takes a few seconds.\n",
        "%time nlp = spacy.load('en_core_web_md')\n",
        "# process a sentence using the model\n",
        "mydoc = nlp(\"Canada is a large country\")\n",
        "#Get a vector for individual words\n",
        "#print(doc[0].vector) #vector for 'Canada', the first word in the text \n",
        "print(doc.vector) #Averaged vector for the entire sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx2A7oX5bW66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#What happens when I give a strange word, and try to get its word vector in Spacy?\n",
        "temp = nlp('practicalnlp is a newword')\n",
        "temp[0].vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sExi6uERbW69",
        "colab_type": "text"
      },
      "source": [
        "Well, at least, this is better than throwing an exception! :) \n",
        "\n"
      ]
    }
  ]
}